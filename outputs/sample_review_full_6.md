# Literature Review

## 1. RITA: Group Attention is All You Need for Timeseries Analytics
**Authors**: Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li

**Summary**: the output of our group attention is identical to vanilla self-attentionâ€™s . the error is bounded by exp(2dR) Ai,j exp(2dr) (13) Combining (12) (13), we have oi = N 1 j=0 ePi,k si evj = eoi .

## 2. All the attention you need: Global-local, spatial-channel attention for image retrieval
**Authors**: Chull Hwan Song, Hye Joo Han, Yannis Avrithis

**Summary**: our best model trained on GLDv2-clean outperforms by a large margin . our baseline is lower than , because our dimensinality is 512 . the most impor- tant comparison is with SOLAR , also based on self- attention .

## 3. Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models
**Authors**: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini

**Summary**: dropping the last layers from the 7B and 13B Llama2 models leads to much lower drops in performance than dropping the MLP layers . removing 33% of attention layers leads to an 18% speedup in a 13B model at the cost of a 1.8% drop in average performance .

