# Literature Review

## 1. Lecture Notes: Optimization for Machine Learning
**Authors**: Elad Hazan

**Summary**: These lecture notes by Elad Hazan cover optimization techniques for machine learning. Key topics include:

*   **Introduction to Optimization Problems:** Discusses examples like Empirical Risk Minimization, Matrix Completion, and Learning in Linear Dynamical Systems. It also addresses the computational hardness of mathematical programming.
*   **Basic Concepts:** Covers convexity, projections onto convex sets, optimality conditions, and solution concepts for non-convex optimization.
*   **Gradient Descent:** Explores gradient descent with Polyak stepsize.
*   **Stochastic Gradient Descent (SGD):** Focuses on SGD and its application to training feedforward neural networks.
*   **Generalization and Non-Smooth Optimization:** Discusses minimizing regret, its implication for generalization, and

Unable to generate summary after 3 attempts.

## 2. An Optimal Control View of Adversarial Machine Learning
**Authors**: Xiaojin Zhu

**Summary**: This paper proposes framing adversarial machine learning as an optimal control problem. The machine learning model is treated as a dynamical system, adversarial attacks as control inputs, and the adversary's objectives (harm and stealth) as control costs. This framework encompasses various attack types and suggests leveraging control theory and reinforcement learning techniques for adversarial defense and understanding.

Unable to generate summary after 3 attempts.

## 3. Minimax deviation strategies for machine learning and recognition with short learning samples
**Authors**: Michail Schlesinger, Evgeniy Vodolazskiy

**Summary**: Unable to generate summary after 3 attempts.

This paper investigates decision-making strategies, defining "improper" strategies as those always outperformed by another. The key finding, **Theorem 1, proves that every strategy is either Bayesian (minimizing expected risk with respect to a weight function) or improper, but not both.** This is demonstrated using a duality theorem applied to a function defined by risk and weight functions, and proven through four supporting propositions.

